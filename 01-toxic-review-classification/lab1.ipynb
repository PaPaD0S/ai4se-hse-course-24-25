{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bfba8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f6573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8857933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.20.0 (from -r requirements.txt (line 1))\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate==0.4.2 (from -r requirements.txt (line 2))\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting numpy==1.26.4 (from -r requirements.txt (line 3))\n",
      "  Downloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting scikit-learn==1.5.1 (from -r requirements.txt (line 4))\n",
      "  Downloading scikit_learn-1.5.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (12 kB)\n",
      "Collecting tqdm==4.66.4 (from -r requirements.txt (line 5))\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting filelock (from datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading pyarrow-21.0.0-cp39-cp39-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading pandas-2.3.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting requests>=2.32.2 (from datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting xxhash (from datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading xxhash-3.6.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading multiprocess-0.70.18-py39-none-any.whl.metadata (7.5 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading aiohttp-3.13.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in ./lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 1)) (25.0)\n",
      "Collecting pyyaml>=5.1 (from datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading pyyaml-6.0.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn==1.5.1->-r requirements.txt (line 4))\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn==1.5.1->-r requirements.txt (line 4))\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.5.1->-r requirements.txt (line 4))\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading frozenlist-1.8.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading multidict-6.7.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading propcache-0.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading yarl-1.22.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./lib/python3.9/site-packages (from multidict<7.0,>=4.5->aiohttp->datasets==2.20.0->-r requirements.txt (line 1)) (4.15.0)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.21.2->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface-hub>=0.21.2->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting shellingham (from huggingface-hub>=0.21.2->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub>=0.21.2->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->huggingface-hub>=0.21.2->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->huggingface-hub>=0.21.2->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub>=0.21.2->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.21.2->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading charset_normalizer-3.4.4-cp39-cp39-macosx_10_9_universal2.whl.metadata (37 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.21.2->datasets==2.20.0->-r requirements.txt (line 1)) (1.3.1)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading multiprocess-0.70.17-py39-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./lib/python3.9/site-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0->-r requirements.txt (line 1)) (1.17.0)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface-hub>=0.21.2->datasets==2.20.0->-r requirements.txt (line 1))\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m253.4 kB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "Downloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m630.4 kB/s\u001b[0m  \u001b[33m0:00:22\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.1-cp39-cp39-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m636.0 kB/s\u001b[0m  \u001b[33m0:00:17\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Downloading aiohttp-3.13.2-cp39-cp39-macosx_11_0_arm64.whl (490 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.7.0-cp39-cp39-macosx_11_0_arm64.whl (44 kB)\n",
      "Downloading yarl-1.22.0-cp39-cp39-macosx_11_0_arm64.whl (94 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading frozenlist-1.8.0-cp39-cp39-macosx_11_0_arm64.whl (50 kB)\n",
      "Downloading huggingface_hub-1.2.3-py3-none-any.whl (520 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m356.5 kB/s\u001b[0m  \u001b[33m0:00:07\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading propcache-0.4.1-cp39-cp39-macosx_11_0_arm64.whl (47 kB)\n",
      "Downloading pyarrow-21.0.0-cp39-cp39-macosx_12_0_arm64.whl (31.2 MB)\n",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/31.2 MB\u001b[0m \u001b[31m498.6 kB/s\u001b[0m eta \u001b[36m0:01:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py\", line 459, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py\", line 503, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 85, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_internal/commands/install.py\", line 415, in run\n",
      "    preparer.prepare_linked_requirements_more(\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 570, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_internal/operations/prepare.py\", line 479, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_internal/network/download.py\", line 184, in batch\n",
      "    filepath, content_type = self(link, location)\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_internal/network/download.py\", line 195, in __call__\n",
      "    self._process_response(download, resp)\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_internal/network/download.py\", line 212, in _process_response\n",
      "    for chunk in chunks:\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_internal/cli/progress_bars.py\", line 67, in _rich_download_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_internal/network/utils.py\", line 65, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 587, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/papa_d0s/study/ai/ai4se-hse-course-24-25/01-toxic-review-classification/lib/python3.9/site-packages/pip/_vendor/urllib3/response.py\", line 455, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionResetError(54, 'Connection reset by peer')\", ConnectionResetError(54, 'Connection reset by peer'))\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting black==22.3.0 (from -r requirements_dev.txt (line 1))\n",
      "  Downloading black-22.3.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (45 kB)\n",
      "Collecting flake8==4.0.1 (from -r requirements_dev.txt (line 2))\n",
      "  Downloading flake8-4.0.1-py2.py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting flake8-builtins==1.5.3 (from -r requirements_dev.txt (line 3))\n",
      "  Downloading flake8_builtins-1.5.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting flake8-comprehensions==3.10.0 (from -r requirements_dev.txt (line 4))\n",
      "  Downloading flake8_comprehensions-3.10.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting flake8-docstrings==1.5.0 (from -r requirements_dev.txt (line 5))\n",
      "  Downloading flake8_docstrings-1.5.0-py2.py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting flake8-import-order==0.18.1 (from -r requirements_dev.txt (line 6))\n",
      "  Downloading flake8_import_order-0.18.1-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting flake8-pep585==0.1.5.1 (from -r requirements_dev.txt (line 7))\n",
      "  Downloading flake8_pep585-0.1.5.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting flake8-quotes==2.1.1 (from -r requirements_dev.txt (line 8))\n",
      "  Downloading flake8-quotes-2.1.1.tar.gz (12 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting flakeheaven==3.0.0 (from -r requirements_dev.txt (line 9))\n",
      "  Downloading flakeheaven-3.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting click>=8.0.0 (from black==22.3.0->-r requirements_dev.txt (line 1))\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in ./lib/python3.9/site-packages (from black==22.3.0->-r requirements_dev.txt (line 1)) (4.4.0)\n",
      "Collecting pathspec>=0.9.0 (from black==22.3.0->-r requirements_dev.txt (line 1))\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting mypy-extensions>=0.4.3 (from black==22.3.0->-r requirements_dev.txt (line 1))\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in ./lib/python3.9/site-packages (from black==22.3.0->-r requirements_dev.txt (line 1)) (4.15.0)\n",
      "Collecting tomli>=1.1.0 (from black==22.3.0->-r requirements_dev.txt (line 1))\n",
      "  Downloading tomli-2.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mccabe<0.7.0,>=0.6.0 (from flake8==4.0.1->-r requirements_dev.txt (line 2))\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pycodestyle<2.9.0,>=2.8.0 (from flake8==4.0.1->-r requirements_dev.txt (line 2))\n",
      "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl.metadata (31 kB)\n",
      "Collecting pyflakes<2.5.0,>=2.4.0 (from flake8==4.0.1->-r requirements_dev.txt (line 2))\n",
      "  Downloading pyflakes-2.4.0-py2.py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pydocstyle>=2.1 (from flake8-docstrings==1.5.0->-r requirements_dev.txt (line 5))\n",
      "  Downloading pydocstyle-6.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: setuptools in ./lib/python3.9/site-packages (from flake8-import-order==0.18.1->-r requirements_dev.txt (line 6)) (58.0.4)\n",
      "Collecting colorama (from flakeheaven==3.0.0->-r requirements_dev.txt (line 9))\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting entrypoints (from flakeheaven==3.0.0->-r requirements_dev.txt (line 9))\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pygments in ./lib/python3.9/site-packages (from flakeheaven==3.0.0->-r requirements_dev.txt (line 9)) (2.19.2)\n",
      "Collecting toml (from flakeheaven==3.0.0->-r requirements_dev.txt (line 9))\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting urllib3 (from flakeheaven==3.0.0->-r requirements_dev.txt (line 9))\n",
      "  Using cached urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting snowballstemmer>=2.2.0 (from pydocstyle>=2.1->flake8-docstrings==1.5.0->-r requirements_dev.txt (line 5))\n",
      "  Downloading snowballstemmer-3.0.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Downloading black-22.3.0-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading flake8-4.0.1-py2.py3-none-any.whl (64 kB)\n",
      "Downloading flake8_builtins-1.5.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flake8_comprehensions-3.10.0-py3-none-any.whl (7.3 kB)\n",
      "Downloading flake8_docstrings-1.5.0-py2.py3-none-any.whl (5.5 kB)\n",
      "Downloading flake8_import_order-0.18.1-py2.py3-none-any.whl (15 kB)\n",
      "Downloading flake8_pep585-0.1.5.1-py3-none-any.whl (9.8 kB)\n",
      "Downloading flakeheaven-3.0.0-py3-none-any.whl (45 kB)\n",
      "Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
      "Downloading pyflakes-2.4.0-py2.py3-none-any.whl (69 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading pydocstyle-6.3.0-py3-none-any.whl (38 kB)\n",
      "Downloading snowballstemmer-3.0.1-py3-none-any.whl (103 kB)\n",
      "Downloading tomli-2.3.0-py3-none-any.whl (14 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Building wheels for collected packages: flake8-quotes\n",
      "  Building wheel for flake8-quotes (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flake8-quotes: filename=flake8_quotes-2.1.1-py3-none-any.whl size=8459 sha256=64f2c678ede9aaadc17e3e2861f9448b34304bd71ba0fee45bac97de590f4a5b\n",
      "  Stored in directory: /Users/papa_d0s/Library/Caches/pip/wheels/14/5b/c3/53f1ed424a845595356c653849154c0e72125a69381229fba1\n",
      "Successfully built flake8-quotes\n",
      "Installing collected packages: mccabe, urllib3, tomli, toml, snowballstemmer, pyflakes, pycodestyle, pathspec, mypy-extensions, flake8-pep585, entrypoints, colorama, click, pydocstyle, flake8-import-order, flake8, black, flakeheaven, flake8-quotes, flake8-docstrings, flake8-comprehensions, flake8-builtins\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [flake8-builtins] [flakeheaven]\n",
      "\u001b[1A\u001b[2KSuccessfully installed black-22.3.0 click-8.1.8 colorama-0.4.6 entrypoints-0.4 flake8-4.0.1 flake8-builtins-1.5.3 flake8-comprehensions-3.10.0 flake8-docstrings-1.5.0 flake8-import-order-0.18.1 flake8-pep585-0.1.5.1 flake8-quotes-2.1.1 flakeheaven-3.0.0 mccabe-0.6.1 mypy-extensions-1.1.0 pathspec-0.12.1 pycodestyle-2.8.0 pydocstyle-6.3.0 pyflakes-2.4.0 snowballstemmer-3.0.1 toml-0.10.2 tomli-2.3.0 urllib3-2.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r \"requirements.txt\"\n",
    "!pip install -r \"requirements_dev.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "195fe487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0154ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df.drop_duplicates() # Удаляем дубликаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "301358ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df['message'] = df['message'].apply(lambda x: re.sub(r'http\\S+', '\\'link\\'',  x)) # Удаляем URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9826b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\n",
    "                       \"can't\": \"cannot\", \"'cause\": \"because\",\n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                       \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
    "                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
    "                       \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
    "                       \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\",\n",
    "                       \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\",\n",
    "                       \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
    "                       \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                       \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                       \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
    "                       \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
    "                       \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
    "                       \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                       \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
    "                       \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "                       \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
    "                       \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\",\n",
    "                       \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
    "                       \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                       \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
    "                       \"here's\": \"here is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                       \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
    "                       \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "                       \"what'll\": \"what will\",\n",
    "                       \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "                       \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n",
    "                       \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n",
    "                       \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\",\n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
    "                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
    "                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                       \"you're\": \"you are\", \"you've\": \"you have\", \"aint\": \"is not\", \"arent\": \"are not\",\n",
    "                       \"cant\": \"cannot\", \"cause\": \"because\",\n",
    "                       \"couldve\": \"could have\", \"couldnt\": \"could not\",\n",
    "                       \"didnt\": \"did not\", \"doesnt\": \"does not\",\n",
    "                       \"dont\": \"do not\", \"hadnt\": \"had not\", \"hasnt\": \"has not\",\n",
    "                       \"havent\": \"have not\", \"howdy\": \"how do you\",\n",
    "                       \"its\": \"it is\", \"lets\": \"let us\", \"maam\": \"madam\", \"maynt\": \"may not\",\n",
    "                       \"mightve\": \"might have\", \"mightnt\": \"might not\",\n",
    "                       \"mightntve\": \"might not have\", \"mustve\": \"must have\",\n",
    "                       \"mustnt\": \"must not\", \"mustntve\": \"must not have\",\n",
    "                       \"neednt\": \"need not\", \"needntve\": \"need not have\",\n",
    "                       \"oclock\": \"of the clock\", \"oughtnt\": \"ought not\",\n",
    "                       \"shouldve\": \"should have\", \"shouldnt\": \"should not\",\n",
    "                       \"werent\": \"were not\", \"youre\": \"you are\", \"yall\": \"you all\",\n",
    "                       \"youve\": \"you have\"}\n",
    "\n",
    "def full_from_contraction(text):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\", \"'\"]\n",
    "\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "        text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "df['message'] = df['message'].apply(lambda x: x.lower())\n",
    "df['message'] = df['message'].apply(full_from_contraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1156d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def delete_duplicate(dup_string):\n",
    "  return ''.join([x for x,y in groupby(dup_string)])\n",
    "\n",
    "df['message'] = df['message'].apply(delete_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "410b48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_special_sym(text):\n",
    "    pattern = re.compile('([^\\s\\w]|_|\\\\n)+')\n",
    "    return pattern.sub(' ', text)\n",
    "\n",
    "df['message'] = df['message'].apply(del_special_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07c90927",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_PATTERNS = {\n",
    "    ' fuck ':\n",
    "        [\n",
    "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
    "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
    "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
    "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
    "            'feck ', ' fux ', 'f\\*\\*',\n",
    "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
    "\n",
    "        ],\n",
    "\n",
    "    ' crap ':\n",
    "        [\n",
    "            ' (c)(r|[^a-z0-9 ])(a|[^a-z0-9 ])(p|[^a-z0-9 ])([^ ])*',\n",
    "            ' (c)([^a-z]*)(r)([^a-z]*)(a)([^a-z]*)(p)',\n",
    "            ' c[!@#\\$%\\^\\&\\*]*r[!@#\\$%\\^&\\*]*p', 'cr@p', ' c r a p',\n",
    "\n",
    "        ],\n",
    "\n",
    "    ' ass ':\n",
    "        [\n",
    "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n",
    "                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
    "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n",
    "        ],\n",
    "\n",
    "    ' bitch ':\n",
    "        [\n",
    "            'bitches', ' b[w]*i[t]*ch', ' b!tch',\n",
    "            ' bi\\+ch', ' b!\\+ch', ' (b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
    "            ' biatch', ' bi\\*\\*h', ' bytch', 'b i t c h'\n",
    "        ],\n",
    "    \n",
    "    ' ass hole ':\n",
    "        [\n",
    "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n",
    "        ],\n",
    "\n",
    "    ' bastard ':\n",
    "        [\n",
    "            'ba[s|z]+t[e|a]+rd'\n",
    "        ],\n",
    "\n",
    "    ' transgender':\n",
    "        [\n",
    "            'transgender'\n",
    "        ],\n",
    "\n",
    "    ' gay ':\n",
    "        [\n",
    "            'gay', 'homo'\n",
    "        ],\n",
    "\n",
    "    ' cock ':\n",
    "        [\n",
    "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
    "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
    "        ],\n",
    "\n",
    "    ' dick ':\n",
    "        [\n",
    "            ' dick[^aeiou]', 'd i c k'\n",
    "        ],\n",
    "\n",
    "    ' suck ':\n",
    "        [\n",
    "            'sucker', 'sucks', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', '5uck', 's u c k'\n",
    "        ],\n",
    "\n",
    "    ' cunt ':\n",
    "        [\n",
    "            'cunt', 'c u n t'\n",
    "        ],\n",
    "\n",
    "    ' bull shit ':\n",
    "        [\n",
    "            'bullsh\\*t', 'bull\\$hit', 'bull sh.t'\n",
    "        ],\n",
    "\n",
    "    ' jerk ':\n",
    "        [\n",
    "            'jerk'\n",
    "        ],\n",
    "\n",
    "    ' idiot ':\n",
    "        [\n",
    "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots' 'i d i o t'\n",
    "        ],\n",
    "\n",
    "    ' dumb ':\n",
    "        [\n",
    "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
    "        ],\n",
    "\n",
    "    ' shit ':\n",
    "        [\n",
    "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t', 'sh\\*tty',\n",
    "            'sh\\*ty', 'sh\\*t'\n",
    "        ],\n",
    "\n",
    "    ' shit hole ':\n",
    "        [\n",
    "            'shythole', 'sh.thole'\n",
    "        ],\n",
    "\n",
    "    ' retard ':\n",
    "        [\n",
    "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
    "        ],\n",
    "\n",
    "    ' rape ':\n",
    "        [\n",
    "            'raped'\n",
    "        ],\n",
    "\n",
    "    ' dumb ass':\n",
    "        [\n",
    "            'dumbass', 'dubass'\n",
    "        ],\n",
    "\n",
    "    ' ass head':\n",
    "        [\n",
    "            'butthead'\n",
    "        ],\n",
    "\n",
    "    ' sex ':\n",
    "        [\n",
    "            'sexy', 's3x', 'sexuality'\n",
    "        ],\n",
    "\n",
    "    ' nigger ':\n",
    "        [\n",
    "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
    "        ],\n",
    "\n",
    "    ' shut the fuck up':\n",
    "        [\n",
    "            ' stfu' '^stfu'\n",
    "        ],\n",
    "\n",
    "    ' for your fucking information':\n",
    "        [\n",
    "            ' fyfi', '^fyfi'\n",
    "        ],\n",
    "    ' get the fuck off':\n",
    "        [\n",
    "            'gtfo', '^gtfo'\n",
    "        ],\n",
    "\n",
    "    ' oh my fucking god ':\n",
    "        [\n",
    "            ' omfg', '^omfg'\n",
    "        ],\n",
    "\n",
    "    ' what the hell ':\n",
    "        [\n",
    "            ' wth', '^wth'\n",
    "        ],\n",
    "\n",
    "    ' what the fuck ':\n",
    "        [\n",
    "            ' wtf', '^wtf'\n",
    "        ],\n",
    "    ' son of bitch ':\n",
    "        [\n",
    "            ' sob ', '^sob '\n",
    "        ],\n",
    "\n",
    "    ' pussy ':\n",
    "        [\n",
    "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses', '(p)(u|[^a-z0-9 ])(s|[^a-z0-9 ])(s|[^a-z0-9 ])(y)',\n",
    "        ],\n",
    "\n",
    "    ' faggot ':\n",
    "        [\n",
    "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
    "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
    "        ],\n",
    "\n",
    "    ' mother fucker':\n",
    "        [\n",
    "            ' motha f', ' mother f', 'motherucker', ' mofo', ' mf ',\n",
    "        ],\n",
    "\n",
    "    ' whore ':\n",
    "        [\n",
    "            'wh\\*\\*\\*', 'w h o r e'\n",
    "        ],\n",
    "\n",
    "    ' haha ':\n",
    "        [\n",
    "            'ha\\*\\*\\*ha',\n",
    "        ],\n",
    "}\n",
    "\n",
    "def perform_text(text):\n",
    "    for target, patterns in RE_PATTERNS.items():\n",
    "        for pat in patterns:\n",
    "            text = re.sub(pat, target, text)\n",
    "    text = re.sub(r\"[^a-z' ]\", ' ', text)\n",
    "    return text\n",
    "\n",
    "df['message'] = df['message'].apply(perform_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('prepared_data.csv', index=False) # После предобработки сохраните подготовленные данные для дальнейшего использования\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01-toxic-review-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
